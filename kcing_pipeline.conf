# Step 1: retrieve data
input {
    http {
        host => "0.0.0.0"
        port => "8337"
        type => "build"
    }

    http {
        host => "0.0.0.0"
        port => "8338"
        type => "lava"
    }

    # For non-lava labs
    http {
        host => "0.0.0.0"
        port => "8007"
        type => "boot"
    }
}

# Step 2: apply filters and do some magic
filter {

    # Avoid parsing empty stuff
    if [message] =~ /^\s*$/ {
        drop { }
    }

    # Converts input into json
    json {
        source => "message"
    }

    # Checks if json parsing has failed
    # https://discuss.elastic.co/t/logstash-json-parse-error/74000/15
    if "_jsonparsefailure" in [tags] {
        mutate {
            replace => ["type", "error"]
        }
    }

    if [type] == "boot" {
        mutate {
            remove_field => ["fastboot"]
            lowercase => ["boot_result"]
        }
    }

    # Use lava documents to span a boot, it needs to look like kernelci boot.json
    # and the lava document will contain common fields like in boot + test results + target logs
    if [type] == "lava" {
        # If input is coming from http, parse GET params
        if [headers] != "" {
            kv {
                include_keys => ["lab_name"]
                field_split => "&?"
                source => "[headers][request_path]"
            }
        }

        # Parse YAML fields
        ruby {
            init => "require 'yaml'; require 'json';"
            code => "
                     boot_time = ''
                     boot_result = ''

                     tests = []
                     results = event.get('results')
                     results.each do |suite, values|
                        sets = YAML.load(values)
                        sets.each do |set|

                            # Remove unwanted data (metadata keeps changing and it messes up with ES)
                            set.delete('metadata')
                            set.delete('job')
                            set.delete('url')

                            # Truncate datatime precision
                            set['logged'] = set['logged'][0..-10]
                            # Get boot info
                            if set['name'] == 'auto-login-action' then
                                boot_result = set['result']
                                boot_time = set['measurement']
                            end
                            tests.push(set)
                        end
                     end
                     event.set('[@metadata][results]', tests.to_json)

                     # Truncate second precision to 3 digits, instead of 6
                     logs = YAML.load(event.get('log'))
                     new_logs = []
                     lineno = 1
                     logs.each do |_log|
                        _log['dt'] = _log['dt'][0..-4]
                        _log['lineno'] = lineno
                        new_logs.push(_log)
                        lineno += 1
                     end
                     event.set('[@metadata][logs]', new_logs.to_json)

                     # Extract common fields to later span a boot doc
                     definition = YAML.load(event.get('definition'))
                     m = definition['metadata'];
                     common_fields = {
                        'git_commit':   m['git.commit'],
                        'git_branch':   m['git.branch'],
                        'git_describe': m['git.describe'],
                        'job': m['kernel.tree'],
                        'kernel': m['kernel.version'],
                        'defconfig_full': m['kernel.defconfig'],
                        'build_environment': m['job.build_environment'],
                        'board': m['device.type'],
                        'arch': m['job.arch'],
                        'endian': m['kernel.endian'],
                        'boot_result': boot_result.downcase,
                        'boot_time': boot_time
                     };
                     common_fields['lab_name'] = event.get('lab_name')
                     event.set('[@metadata][common]', common_fields)

                     # Get job_id
                     event.set('[@metadata][job_id]', event.get('id'))
                     "
        }

        # Now that we got all needed common data, let's split the lava file into
        # two: one to become boot.json and another to become test.json
        mutate {
            remove_field => ["id", "description", "version", "status_string", "definition",
                             "start_time", "boot_log_html", "failure_comment", "results",
                             "metadata", "actual_device_id", "@version", "log", "submit_time",
                             "end_time", "status", "submitter_username", "host"]
        }

        # Add common fields
        mutate { add_field    => { "common" => "%{[@metadata][common]}" } }
        json   { source       =>   "common" }
        mutate { remove_field =>  ["common"] }

        # Span boot, log and test doc
        clone {
            clones => ["boot", "log", "test"]
        }

        if [type] == "test" {
            mutate { add_field => { "job_id" => "%{[@metadata][job_id]}" }}
            mutate { add_field => { "results" => "%{[@metadata][results]}" } }
            json {
                source => "results"
                target => "results"
            }

            split {
                field => "results"
                remove_field => ["@version"]
            }

            # Bring nested field 'results' one level up (couldn't find a better way to do this :/)
            mutate {
                add_field => {
                    "result" => "%{[results][result]}"
                    "unit" => "%{[results][unit]}"
                    "name" => "%{[results][name]}"
                    "measurement" => "%{[results][measurement]}"
                    "log_start_line" => "%{[results][log_start_line]}"
                    "log_end_line" => "%{[results][log_end_line]}"
                    "suite" => "%{[results][suite]}"
                    "level" => "%{[results][level]}"
                    "logged" => "%{[results][logged]}"
                    "id" => "%{[results][id]}"
                }

                remove_field => ["results"]
            }

        }

        if [type] == "log" {
            mutate { add_field => { "job_id" => "%{[@metadata][job_id]}" }}
            mutate { add_field => { "logs" => "%{[@metadata][logs]}" } }
            json {
                source => "logs"
                target => "logs"
            }
            split {
                field => "logs"
                remove_field => ["@version"]
            }
            # Bring nested field 'results' one level up (couldn't find a better way to do this :/)
            mutate {
                add_field => {
                    "lvl" => "%{[logs][lvl]}"
                    "dt" => "%{[logs][dt]}"
                    "msg" => "%{[logs][msg]}"
                    "lineno" => "%{[logs][lineno]}"
                }

                remove_field => ["logs"]
            }

        }
    }

    # Get rid of lava*.json
    if [type] == "lava" {
        drop { }
    }
 
    # Remove the generic type field (used only to determine the index type)
    mutate { add_field => { "[@metadata][index_type]" => "%{type}" } }

    # Leave "message" field in case of errors
    if [type] != "error" {
        mutate { remove_field => ["type", "headers", "message", "@version", "token"] }
    }
}

# Step 3: submit it to ES
output {
    if [@metadata][index_type] != "error" {
        elasticsearch {
            hosts => ["127.0.0.1:9200"]
            index => "%{[@metadata][index_type]}-%{+YYYY.MM.dd}"
        }

        # Debugging purposes
        file {
            path => "/tmp/%{[@metadata][index_type]}-%{+YYYY.MM.dd}"
            write_behavior => "overwrite"
        }
    }

    # Append errors to specific file
    if [@metadata][index_type] == "error" {
        file {
            path => "/tmp/%{[@metadata][index_type]}-%{+YYYY.MM.dd}"
            write_behavior => "overwrite"
        }
    }
}
