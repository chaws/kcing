# Step 1: retrieve data
input {
    http {
        host => "0.0.0.0"
        port => "8337"
        type => "build"
    }

    http {
        host => "0.0.0.0"
        port => "8338"
        type => "lava"
    }

    # For non-lava labs
    http {
        host => "0.0.0.0"
        port => "8007"
        type => "boot"
    }
}

# Step 2: apply filters and do some magic
filter {

    # Avoid parsing empty stuff
    if [message] =~ /^\s*$/ {
        drop { }
    }

    # Converts input into json
    json {
        source => "message"
    }

    # Checks if json parsing has failed
    # https://discuss.elastic.co/t/logstash-json-parse-error/74000/15
    if "_jsonparsefailure" in [tags] {
        mutate {
            replace => ["type", "error"]
        }
    }

    if [type] == "boot" {
        mutate { remove_field => ["fastboot"] }
    }

    # Use lava documents to span a boot, it needs to look like kernelci boot.json
    # and the lava document will contain common fields like in boot + test results + target logs
    if [type] == "lava" {
        # If input is coming from http, parse GET params
        if [headers] != "" {
            kv {
                include_keys => ["lab_name"]
                field_split => "&?"
                source => "[headers][request_path]"
            }
        }

        # Parse YAML fields
        ruby {
            init => "require 'yaml'; require 'json';"
            code => "
                     boot_time = ''
                     boot_result = ''

                     tests = []
                     results = event.get('results')
                     results.each do |suite, values|
                        sets = YAML.load(values)
                        sets.each do |set|
                            set.delete('metadata')
                            set.delete('job')
                            set.delete('url')

                            # Get boot info
                            if set['name'] == 'auto-login-action' then
                                boot_result = set['result']
                                boot_time = set['measurement']
                            end
                            tests.push(set)
                        end
                     end
                     event.set('[@metadata][results]', tests.to_json)

                     # Process logs to contain target only messages
                     logs = YAML.load(event.get('log'))
                     target_logs = []
                     lineno = 1
                     logs.each do |_log|
                        if _log['lvl'] == 'target' then
                            target_logs.push({
                                'dt': _log['dt'][0..-4],
                                'msg': _log['msg'],
                                'lineno': lineno
                            })    
                        end
                        lineno += 1
                     end
                     event.set('[@metadata][target_logs]', target_logs.to_json)

                     # Extract common fields to later span a boot doc
                     definition = YAML.load(event.get('definition'))
                     m = definition['metadata'];
                     common_fields = {
                        'git_url':      m['git.url'],
                        'git_commit':   m['git.commit'],
                        'git_branch':   m['git.branch'],
                        'git_describe': m['git.describe'],
                        'dtb_url': m['job.dtb_url'],
                        'job': m['kernel.tree'],
                        'kernel': m['kernel.version'],
                        'defconfig': m['kernel.defconfig'],
                        'file_server_resource': m['job.file_server_resource'],
                        'build_environment': m['job.build_environment'],
                        'board': m['device.type'],
                        'arch': m['job.arch'],
                        'mach': m['platform.mach'],
                        'kernel_image': m['job.kernel_image'],
                        'initrd': m['job.initrd_url'],
                        'endian': m['kernel.endian'],
                        'boot_result': boot_result.downcase,
                        'boot_time': boot_time
                     };
                     event.set('[@metadata][common]', common_fields)

                     # Get job_id
                     event.set('[@metadata][job_id]', event.get('id'))
                     "
        }

        # Now that we got all needed common data, let's split the lava file into
        # two: one to become boot.json and another to become test.json
        mutate {
            remove_field => ["id", "description", "version", "status_string", "definition",
                             "start_time", "boot_log_html", "failure_comment", "results",
                             "metadata", "actual_device_id", "@version", "log", "submit_time",
                             "end_time", "status", "submitter_username"]
        }

        # Span boot doc
        clone {
            clones => ["boot"]
        }

        # Add common fields
        mutate { add_field    => { "common" => "%{[@metadata][common]}" } }
        json   { source       =>   "common" }
        mutate { remove_field =>  ["common"] }

        if [type] == "lava" {
            mutate { add_field => { "results" => "%{[@metadata][results]}" } }
            json {
                source => "results"
                target => "results"
            }

            mutate { add_field => { "logs" => "%{[@metadata][target_logs]}" } }
            json {
                source => "logs"
                target => "logs"
            }

            mutate { add_field => { "job_id" => "%{[@metadata][job_id]}" }}
        }
    }
 
    # Remove the generic type field (used only to determine the index type)
    mutate { add_field => { "[@metadata][index_type]" => "%{type}" } }

    # Leave "message" field in case of errors
    if [type] != "error" {
        mutate { remove_field => ["type", "headers", "message", "@version", "token"] }
    }
}

# Step 3: submit it to ES
output {
    if [@metadata][index_type] != "error" {
        elasticsearch {
            hosts => ["127.0.0.1:9200"]
            index => "%{[@metadata][index_type]}-%{+YYYY.MM.dd}"
        }

        # Debugging purposes
        file {
            path => "/tmp/%{[@metadata][index_type]}-%{+YYYY.MM.dd}"
            write_behavior => "overwrite"
        }
    }

    # Append errors to specific file
    if [@metadata][index_type] == "error" {
        file {
            path => "/tmp/%{[@metadata][index_type]}-%{+YYYY.MM.dd}"
        }
    }
}
