# Step 1: retrieve data
input {
    http {
        host => "0.0.0.0"
        port => "8337"
        type => "build"
    }

    http {
        host => "0.0.0.0"
        port => "8338"
        type => "lava"
    }

    # For non-lava labs
    http {
        host => "0.0.0.0"
        port => "8007"
        type => "boot"
    }
}

# Step 2: apply filters and do some magic
filter {

    # Avoid parsing empty stuff
    if [message] =~ /^\s*$/ {
        drop { }
    }

    # Converts input into json
    json {
        source => "message"
    }

    mutate {
        remove_field => ["message"]
    }

    # Checks if json parsing has failed
    # https://discuss.elastic.co/t/logstash-json-parse-error/74000/15
    if "_jsonparsefailure" in [tags] {
        mutate {
            replace => ["type", "error"]
        }
    }

    if [type] == "boot" {
        mutate {
            remove_field => ["fastboot"]
            lowercase => ["boot_result"]
            add_field => {"git_describe" => "%{kernel}"}
        }
    }

    # Use lava documents to span a boot, it needs to look like kernelci boot.json
    # and the lava document will contain common fields like in boot + test results + target logs
    if [type] == "lava" {
        # If input is coming from http, parse GET params
        if [headers] != "" {
            kv {
                include_keys => ["lab_name"]
                field_split => "&?"
                source => "[headers][request_path]"
            }
        }

        # Make sure it contains necessary fields
        if [results] == "" or [definition] == "" or [log] == "" or [lab_name] == "" or [id] == "" {
            mutate {
                replace => ["type", "error"]
            }
        }

        if [type] != "error" {

            # Parse YAML fields
            ruby {
                init => "require 'yaml'; require 'json';"
                code => "
                         boot_time = ''
                         boot_result = ''

                         tests = []
                         results = event.get('results')
                         results.each do |suite, values|
                            sets = YAML.load(values)
                            sets.each do |set|

                                # Remove unwanted data (metadata keeps changing and it messes up with ES)
                                set.delete('metadata')
                                set.delete('job')
                                set.delete('url')

                                # Truncate datatime precision
                                if set['logged'] != nil then
                                    set['logged'] = set['logged'][0..-10]
                                else
                                    # for lab-bjorn, results don't have 'logged' field
                                    set['logged'] = event.get('@timestamp')
                                end

                                # Fill in gaps for lab-bjorn
                                if set['suite'] == nil then
                                    set['suite'] = 'lava'
                                end
                                if set['measurement'] == nil then
                                    set['measurement'] = 0
                                end
                                if set['log_start_line'] == nil then
                                    set['log_start_line'] = 0
                                    set['log_end_line'] = 0
                                end
                                if set['unit'] == nil then
                                    set['unit'] = ''
                                end
                                if set['level'] == nil then
                                    set['level'] = ''
                                end
                                if set['id'] == nil then
                                    set['id'] = ''
                                end

                                # Get boot info
                                if set['name'] == 'auto-login-action' then
                                    boot_result = set['result']
                                    boot_time = set['measurement']
                                end
                                tests.push(set)
                            end
                         end
                         event.set('results', tests.to_json)

                         # lab-bjorn might not run auto-login-action, so we just add boot
                         if boot_result == '' then
                            boot_result = 'pass'
                            boot_time = 0
                         end

                         # Truncate second precision to 3 digits, instead of 6 and add line number
                         logs = YAML.load(event.get('log'))
                         new_logs = []
                         lineno = 1
                         logs.each do |_log|
                            _log['dt'] = _log['dt'][0..-4]
                            _log['lineno'] = lineno
                            new_logs.push(_log)
                            lineno += 1
                         end
                         event.set('log', new_logs.to_json)

                         # Extract common fields to later span boot, test and log documents
                         definition = YAML.load(event.get('definition'))
                         m = definition['metadata'];
                         event.set('git_commit', m['git.commit'])
                         event.set('git_branch', m['git.branch'])
                         event.set('git_describe', m['git.describe'])
                         event.set('job', m['kernel.tree'])
                         event.set('kernel', m['kernel.version'])
                         event.set('defconfig_full', m['kernel.defconfig'])
                         event.set('build_environment', m['job.build_environment'])
                         event.set('board', m['device.type'])
                         event.set('arch', m['job.arch'])
                         event.set('file_server_resource', m['job.file_server_resource'])
                         event.set('mach', m['platform.mach'])
                         event.set('endian', m['kernel.endian'])
                         event.set('platform_name', m['platform.name'])
                         event.set('boot_result', boot_result.downcase)
                         event.set('boot_time', boot_time)

                         # Get job_id
                         event.set('[@metadata][job_id]', event.get('id'))
                         "
            }

            # Now that we got all needed common data, let's split the lava file into
            # two: one to become boot.json and another to become test.json
            mutate {
                remove_field => ["id", "description", "version", "status_string", "definition",
                                 "start_time", "boot_log_html", "failure_comment",
                                 "metadata", "actual_device_id", "@version", "submit_time",
                                 "end_time", "status", "submitter_username", "host"]
            }

            # Span boot, log and test doc
            clone {
                clones => ["boot", "log", "test"]
            }

            if [type] == "boot" {
                mutate {
                    remove_field => ["log", "results"]
                }
            }

            if [type] == "test" {
                mutate { 
                    add_field => { "job_id" => "%{[@metadata][job_id]}" }
                    remove_field => ["log"]
                }
                json {
                    source => "results"
                    target => "results"
                }

                split {
                    field => "results"
                    remove_field => ["@version"]
                }

                # Bring nested field 'results' one level up (couldn't find a better way to do this :/)
                mutate {
                    add_field => {
                        "result" => "%{[results][result]}"
                        "unit" => "%{[results][unit]}"
                        "name" => "%{[results][name]}"
                        "measurement" => "%{[results][measurement]}"
                        "log_start_line" => "%{[results][log_start_line]}"
                        "log_end_line" => "%{[results][log_end_line]}"
                        "suite" => "%{[results][suite]}"
                        "level" => "%{[results][level]}"
                        "logged" => "%{[results][logged]}"
                        "id" => "%{[results][id]}"
                    }

                    remove_field => ["results"]
                }

            }

            if [type] == "log" {
                mutate { 
                    add_field => { "job_id" => "%{[@metadata][job_id]}" }
                    remove_field => ["results"]
                }
                json {
                    source => "log"
                    target => "log"
                }
                split {
                    field => "log"
                    remove_field => ["@version"]
                }
                # Bring nested field 'results' one level up (couldn't find a better way to do this :/)
                mutate {
                    add_field => {
                        "lvl" => "%{[log][lvl]}"
                        "dt" => "%{[log][dt]}"
                        "msg" => "%{[log][msg]}"
                        "lineno" => "%{[log][lineno]}"
                    }

                    remove_field => ["log"]
                }
            }
        }
    }

    # Get rid of lava*.json
    if [type] == "lava" {
        drop { }
    }
 
    # Remove the generic type field (used only to determine the index type)
    mutate { add_field => { "[@metadata][index_type]" => "%{type}" } }

    # Leave "message" field in case of errors
    if [type] != "error" {
        mutate { remove_field => ["type", "headers", "@version", "token"] }
    }
}

# Step 3: submit it to ES
output {
    if [@metadata][index_type] != "error" {
        elasticsearch {
            hosts => ["127.0.0.1:9200"]
            index => "%{[@metadata][index_type]}-%{+YYYY.MM.dd}"
        }

        # Debugging purposes
        #file {
        #    path => "/tmp/%{[@metadata][index_type]}-%{+YYYY.MM.dd}"
        #    write_behavior => "overwrite"
        #}
    }

    # Append errors to specific file
    if [@metadata][index_type] == "error" {
        file {
            path => "/tmp/%{[@metadata][index_type]}-%{+YYYY.MM.dd}"
        }
    }
}
