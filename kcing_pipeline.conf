# Step 1: retrieve data
input {
    http {
        host => "0.0.0.0"
        port => "8337"
        type => "build"
    }

    http {
        host => "0.0.0.0"
        port => "8338"
        type => "lava"
    }

    # For non-lava labs
    http {
        host => "0.0.0.0"
        port => "8007"
        type => "boot"
    }
}

# Step 2: apply filters and do some magic
filter {

    # Avoid parsing empty stuff
    if [message] =~ /^\s*$/ {
        drop { }
    }

    # Converts input into json
    json {
        source => "message"
    }

    # Checks if json parsing has failed
    # https://discuss.elastic.co/t/logstash-json-parse-error/74000/15
    if "_jsonparsefailure" in [tags] {
        mutate {
            replace => ["type", "error"]
        }
    }

    # Use lava documents to split into two document types: boot and test
    # the boot document needs to look like kernelci boot document
    # and the test document needs to have all common fields from boot + build, 
    # e.g. job name, lab_name, kernel, git_describe, and so on
    if [type] == "lava" {
        # If input is coming from http, parse GET params
        if [headers] != "" {
            kv {
                include_keys => ["lab_name"]
                field_split => "&?"
                source => "[headers][request_path]"
            }
        }

        # Parse YAML fields
        ruby {
            init => "require 'yaml'; require 'json';"
            code => "definition = YAML.load(event.get('definition'));
                     m = definition['metadata'];
                     common_fields = {
                        'git_url': m['git.url'],
                        'dtb_url': m['job.dtb_url'],
                        'job': m['kernel.tree'],
                        'git_branch': m['git.branch'],
                        'kernel': m['kernel.version'],
                        'defconfig': m['kernel.defconfig'],
                        'file_server_resource': m['job.file_server_resource'],
                        'build_environment': m['job.build_environment'],
                        'board': m['device.type'],
                        'arch': m['job.arch'],
                        'mach': m['platform.mach'],
                        'kernel_image': m['job.kernel_image'],
                        'initrd': m['job.initrd_url'],
                        'git_commit': m['git.commit'],
                        'endian': m['kernel.endian'],
                     };
                     event.set('[@metadata][common]', common_fields);

                     # Variables to create boots
                     boot_time = ''
                     boot_result = ''

                     # We've seen only 'lava' index for results, change here if
                     # new type of results are found
                     results = YAML.load(event.get('results')['lava'])
                     results.each do |result|
                        result.delete('metadata')
                        if result['name'] == 'auto-login-action' then
                            boot_result = result['result']
                            boot_time = result['measurement']
                        end
                     end
                     event.set('[@metadata][results]', results.to_json);

                     # Fields specific for boot
                     boot_fields = {
                        'boot_result': boot_result.downcase,
                        'boot_time': boot_time
                     }
                     event.set('[@metadata][boot_vars]', boot_fields.to_json)

                     # Process logs to contain target only messages
                     logs = YAML.load(event.get('log'))
                     target_logs = []
                     lineno = 1
                     logs.each do |_log|
                        if _log['lvl'] == 'target' then
                            target_logs.push({
                                'dt': _log['dt'][0..-4],
                                'msg': _log['msg'],
                                'lineno': lineno
                            })    
                        end
                        lineno += 1
                     end
                     event.set('[@metadata][target_logs]', target_logs.to_json)
                     "
        }

        # Now that we got all needed common data, let's split the lava file into
        # two: one to become boot.json and another to become test.json

        # Split
        clone {

            # Remove lava fields
            remove_field => ["id", "description", "version", "status_string", "definition",
                             "start_time", "boot_log_html", "failure_comment", "results",
                             "metadata", "actual_device_id", "@version", "log",
                             "submit_time", "end_time", "status", "submitter_username"] 

            # Define types for new clones, after this line, two new
            # events are on the pipeline
            clones => ["boot", "test", "log"]
        }

        if [type] == "boot" or [type] == "test" or [type] == "log" {

            # Add common fields
            mutate { add_field    => { "common" => "%{[@metadata][common]}" } }
            json   { source       =>   "common" }
            mutate { remove_field =>  ["common"] }

            # Create test index
            if [type] == "test" {
                mutate { add_field => { "results" => "%{[@metadata][results]}" } }
                json {
                    source => "results"
                    target => "results"
                }
            }

            # Create boot index
            if [type] == "boot" {
                mutate { add_field    => { "boot_vars" => '%{[@metadata][boot_vars]}' } }
                json   { source       =>   "boot_vars" }
                mutate { remove_field =>  ["boot_vars"] }
            }

            # Create log index
            if [type] == "log" {
                mutate { add_field => { "logs" => "%{[@metadata][target_logs]}" } }
                json {
                    source => "logs"
                    target => "logs"
                }
            }
        }
    }

    # Drop the original event
    if [type] == "lava" {
        drop { }
    }
 
    # Remove the generic type field (used only to determine the index type)
    mutate { add_field => { "[@metadata][index_type]" => "%{type}" } }

    # Leave "message" field in case of errors
    if [type] != "error" {
        mutate { remove_field => ["type", "headers", "message", "@version", "token"] }
    }
}

# Step 3: submit it to ES
output {
    if [@metadata][index_type] != "error" {
        elasticsearch {
            hosts => ["127.0.0.1:9200"]
            index => "%{[@metadata][index_type]}-%{+YYYY.MM.dd}"
        }

        # Debugging purposes
        file {
            path => "/tmp/%{[@metadata][index_type]}-%{+YYYY.MM.dd}"
            write_behavior => "overwrite"
        }
    }

    # Append errors to specific file
    if [@metadata][index_type] == "error" {
        file {
            path => "/tmp/%{[@metadata][index_type]}-%{+YYYY.MM.dd}"
        }
    }
}
