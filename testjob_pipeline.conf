# Step 1: retrieve data
input {
    http {
        host => "0.0.0.0"
        port => "8007"
        type => "boot"
    }
    http {
        host => "0.0.0.0"
        port => "7357"
        type => "testjob"
    }
}

# Step 2: apply filters and do some magic
filter {

    # Avoid parsing empty stuff
    if [message] =~ /^\s*$/ {
        drop { }
    }

    # Converts input into json
    json {
        source => "message"
    }

    # Default type is testjob
    if [type] == "" {
        mutate {
            add_field => {
                "type" => "testjob"
            }
        }
    }

    # If input is coming from http, parse GET params
    if [headers] != "" {
        kv {
            include_keys => ["lab_name"]
            field_split => "&?"
            source => "[headers][request_path]"
        }
    }

    # Stash original document data
    ruby {
        init => "require 'yaml';"
        code => "event.set('[@metadata][definition]', YAML.load(event.get('definition')));

                 # event.get('log') returns a list of dictionaries, which was messing up parsing it to json
                 logs = {'logs' => YAML.load(event.get('log'))}
                 event.set('[@metadata][logs]', logs); 

                 # Same as above
                 results = {'results' => YAML.load(event.get('results')['lava'])}
                 event.set('[@metadata][results]', results);"
    }

    mutate {
        # Remove unwanted fields in testjob
        remove_field => ["definition", "log", "results", "message", "host", "@version", "@timestamp", "health_string", 
                         "submitter_username", "state_string", "state", "health"]

        # Remove fields added by http input plugin
        remove_field => ["headers", "token"]
    } 

    # Add common testjob_id or boot_id field
    mutate {
        add_field => {
            "%{[type]}_id" => "%{id}"
        }
    }

    # Create copies of simple event objects
    clone {

        # Remove testjob fields
        remove_field => ["id", "description", "version", "status_string",
                         "start_time", "boot_log_html", "failure_comment",
                         "metadata", "actual_device_id", "@version", "@timestamp",
                         "host", "submit_time", "end_time", "status", "submitter_username",
                         "priority", "lab_name"] 

        # Define types for new clones
        clones => ["definition", "result", "log"]
    }

    # Build boot document
    if [type] == 'boot' {
        # Remove boot_id field
        mutate {
            remove_field => ["boot_id"]
        }
    }

    # Build testjob document
    if [type] == 'testjob' {
        # Remove testjob_ field
        mutate {
            remove_field => ["testjob_id"]
        }
    }

    # Build definition document
    if [type] == 'definition' {

        # Set definition as field just so we can use it to get json out of it
        mutate {
            add_field => {
                "definition" => "%{[@metadata][definition]}"
            }
        }

        # Parse field as json
        json {
            source => "definition"
        }

        # Remove original string field
        mutate {
            remove_field => ["definition"]
        }
    }

    # Build logs documents
    if [type] == 'log' {
        mutate {
            add_field => {
                "logs" => "%{[@metadata][logs]}"
            }
        }

        # Parse field as json
        json {
            source => "logs"
        }

        split {
            field => "logs"
            remove_field => ["@timestamp", "@version"]
        }

        # Bring nested field 'results' one level up (couldn't find a better way to do this :/)
        mutate {
            add_field => {
                "lvl" => "%{[logs][lvl]}"
                "dt" => "%{[logs][dt]}"
                "msg" => "%{[logs][msg]}"
            }
    
            remove_field => ["logs"]
        }

        # During the conversion, 'msg' got parsed to string, so just parse it back to json
        if [lvl] == 'results' {
            # Drop 'results' logs, since they're already in 'results' documents
            drop { }
        }
    }

    # Build results documents
    if [type] == 'result' {
        mutate {
            add_field => {
                "results" => "%{[@metadata][results]}"
            }
        }

        # Parse field as json
        json {
            source => "results"
        }

        split {
            field => "results"
            remove_field => ["@timestamp", "@version"]
        }

        # Bring nested field 'results' one level up (couldn't find a better way to do this :/)
        mutate {
            add_field => {
                "url" => "%{[results][url]}"
                "result" => "%{[results][result]}"
                "unit" => "%{[results][unit]}"
                "metadata" => "%{[results][metadata]}"
                "name" => "%{[results][name]}"
                "measurement" => "%{[results][measurement]}"
                "log_start_line" => "%{[results][log_start_line]}"
                "log_end_line" => "%{[results][log_end_line]}"
                "suite" => "%{[results][suite]}"
                "level" => "%{[results][level]}"
                "logged" => "%{[results][logged]}"
                "id" => "%{[results][id]}"
            }
    
            remove_field => ["results"]
        }

        # During the conversion, 'metadata' got parsed to string, so just parse it back to json
        json {
            source => "metadata"
            target => "metadata"
        }
    }

    # Remove the generic type field (used only to determine the index type)
    mutate { add_field => { "[@metadata][index_type]" => "%{type}" } }
    mutate { remove_field => ["type"] }
}

# Step 3: submit it to ES
output {
    elasticsearch {
        hosts => ["127.0.0.1:9200"]
        index => "%{[@metadata][index_type]}"
    }

    # Debugging purposes
    file {
        path => "/tmp/%{[@metadata][index_type]}"
        write_behavior => "overwrite"
    }
}
